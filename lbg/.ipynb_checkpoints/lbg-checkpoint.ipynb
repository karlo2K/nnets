{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LBG:\n",
    "    def __init__(self, n_clusters, epsilon=0.00001):\n",
    "        self.n_clusters=n_clusters\n",
    "        self.epsilon=epsilon\n",
    "\n",
    "    def _get_init_cb(self, data):\n",
    "        return np.array([np.mean(data, axis=0)])\n",
    "\n",
    "    def _create_codebook(self, data, e):\n",
    "        return data * (1.0 + e)\n",
    "\n",
    "    def _average_distortion(self, data, codebook, idx):\n",
    "        return np.mean([np.linalg.norm(data - codebook[i], axis=1) ** 2 for i in idx])\n",
    "\n",
    "    def _split_codebook(self, data, codebook, init_distortion):\n",
    "        tmp_cbs = np.empty((0, self.n_features))\n",
    "        for cb in codebook:\n",
    "            for i in range(2):\n",
    "                tmp_cbs = np.append(tmp_cbs, self._create_codebook(codebook, (-1)**i * self.epsilon), axis=0)\n",
    "\n",
    "        codebook = tmp_cbs\n",
    "        distortion = init_distortion\n",
    "        err = 1.0 + self.epsilon;\n",
    "\n",
    "        while err > self.epsilon:\n",
    "            norms = np.array([np.linalg.norm(data - c, axis=1)**2 for c in codebook])\n",
    "            idx_min = np.argmin(norms, axis=0)\n",
    "            idx_map = np.zeros((self.N, self.n_features))\n",
    "            idx_map[range(len(data)), idx_min] = 1\n",
    "\n",
    "            s = (np.sum(idx_map, axis=0, dtype=float)+1e-16).reshape(self.n_features, -1)\n",
    "            codebook = np.dot(idx_map.T, data) / s\n",
    "\n",
    "            prev_distortion = distortion\n",
    "            distortion = self._average_distortion(data, codebook, idx_min)\n",
    "            err = (prev_distortion - distortion) / distortion\n",
    "\n",
    "        return codebook, idx_min\n",
    "\n",
    "    def fit(self, data):\n",
    "        shape = data.shape\n",
    "        self.N = shape[0]\n",
    "        self.n_features=shape[1]\n",
    "        \n",
    "        codebook = self._get_init_cb(data)\n",
    "        distortion = self._average_distortion(data, codebook, np.array([[0]]))\n",
    "\n",
    "        while len(codebook) < self.n_clusters:\n",
    "            codebook, labels = self._split_codebook(data, codebook, distortion)\n",
    "        \n",
    "        self._cluster_centers=codebook\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def predict(self, data):\n",
    "        norms = np.array([np.linalg.norm(data - c, axis=1)**2 for c in self._cluster_centers])\n",
    "        return np.argmin(norms, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "\n",
    "_size_data = 0\n",
    "_dim = 0\n",
    "\n",
    "\n",
    "def generate(data, size_codebook, epsilon=0.00001):\n",
    "    global _size_data, _dim\n",
    "\n",
    "    _size_data = len(data)\n",
    "    assert _size_data > 0\n",
    "\n",
    "    _dim = len(data[0])\n",
    "    assert _dim > 0\n",
    "\n",
    "    codebook = []\n",
    "    codebook_abs_weights = [_size_data]\n",
    "    codebook_rel_weights = [1.0]\n",
    "\n",
    "    c0 = avg_vec_of_vecs(data, _dim, _size_data)\n",
    "    codebook.append(c0)\n",
    "\n",
    "    avg_dist = avg_distortion_c0(c0, data)\n",
    "\n",
    "    while len(codebook) < size_codebook:\n",
    "        codebook, codebook_abs_weights, codebook_rel_weights, avg_dist = split_codebook(data, codebook,\n",
    "                                                                                        epsilon, avg_dist)\n",
    "\n",
    "    return codebook, codebook_abs_weights, codebook_rel_weights\n",
    "\n",
    "\n",
    "def split(data, codebook, epsilon, initial_avg_dist):\n",
    "    new_codevectors = []\n",
    "    for c in codebook:\n",
    "        c1 = new_codevector(c, epsilon)\n",
    "        c2 = new_codevector(c, -epsilon)\n",
    "        new_codevectors.extend((c1, c2))\n",
    "\n",
    "    codebook = new_codevectors\n",
    "    len_codebook = len(codebook)\n",
    "    abs_weights = [0] * len_codebook\n",
    "    rel_weights = [0.0] * len_codebook\n",
    "\n",
    "    avg_dist = 0\n",
    "    err = epsilon + 1\n",
    "    num_iter = 0\n",
    "    while err > epsilon:\n",
    "        closest_c_list = [None] * _size_data    # list that contains the nearest codevector for each input data vector\n",
    "        vecs_near_c = defaultdict(list)         # list with codevector index -> input data vector mapping\n",
    "        vec_idxs_near_c = defaultdict(list)     # list with codevector index -> input data index mapping\n",
    "        for i, vec in enumerate(data):  # for each input vector\n",
    "            min_dist = None\n",
    "            closest_c_index = None\n",
    "            for i_c, c in enumerate(codebook):  # for each codevector\n",
    "                d = euclid_squared(vec, c)\n",
    "                if min_dist is None or d < min_dist:    # found new closest codevector\n",
    "                    min_dist = d\n",
    "                    closest_c_list[i] = c\n",
    "                    closest_c_index = i_c\n",
    "            vecs_near_c[closest_c_index].append(vec)\n",
    "            vec_idxs_near_c[closest_c_index].append(i)\n",
    "\n",
    "        for i_c in range(len_codebook): # for each codevector index\n",
    "            vecs = vecs_near_c.get(i_c) or []   # get its proximity input vectors\n",
    "            num_vecs_near_c = len(vecs)\n",
    "            if num_vecs_near_c > 0:\n",
    "                new_c = avg_vec_of_vecs(vecs, _dim)     # calculate the new center\n",
    "                codebook[i_c] = new_c                   # update in codebook\n",
    "                for i in vec_idxs_near_c[i_c]:          # update in input vector index -> codevector mapping list\n",
    "                    closest_c_list[i] = new_c\n",
    "\n",
    "                # update the weights\n",
    "                abs_weights[i_c] = num_vecs_near_c\n",
    "                rel_weights[i_c] = num_vecs_near_c / _size_data\n",
    "\n",
    "        prev_avg_dist = avg_dist if avg_dist > 0 else initial_avg_dist\n",
    "        avg_dist = avg_distortion_c_list(closest_c_list, data)\n",
    "\n",
    "        err = (prev_avg_dist - avg_dist) / prev_avg_dist\n",
    "        num_iter += 1\n",
    "\n",
    "    return codebook, abs_weights, rel_weights, avg_dist\n",
    "\n",
    "\n",
    "def avg_vec_of_vecs(vecs, dim=None, size=None):\n",
    "    size = size or len(vecs)\n",
    "    dim = dim or len(vecs[0])\n",
    "    avg_vec = [0.0] * dim\n",
    "    for vec in vecs:\n",
    "        for i, x in enumerate(vec):\n",
    "            avg_vec[i] += x / size\n",
    "\n",
    "    return avg_vec\n",
    "\n",
    "\n",
    "def new_codevector(c, e):\n",
    "    return [x * (1.0 + e) for x in c]\n",
    "\n",
    "\n",
    "def avg_distortion_c0(c0, data, size=None):\n",
    "    size = size or _size_data\n",
    "    return reduce(lambda s, d:  s + d / size,\n",
    "                  (euclid_squared(c0, vec)\n",
    "                   for vec in data),\n",
    "                  0.0)\n",
    "\n",
    "\n",
    "def avg_distortion_c_list(c_list, data, size=None):\n",
    "    size = size or _size_data\n",
    "    return reduce(lambda s, d:  s + d / size,\n",
    "                  (euclid_squared(c_i, data[i])\n",
    "                   for i, c_i in enumerate(c_list)),\n",
    "                  0.0)\n",
    "\n",
    "\n",
    "def euclid_squared(a, b):\n",
    "    return sum((x_a - x_b) ** 2 for x_a, x_b in zip(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread, imsave\n",
    "from sklearn.utils import shuffle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'face.png'\n",
    "img = imread(img_path)\n",
    "n_random=1000\n",
    "n_clusters = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols, depth = img.shape\n",
    "img_vect = img.reshape(rows * cols, depth)\n",
    "img_train = shuffle(img_vect)[:n_random]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbg = LBG(n_clusters=n_clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
